{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Feature Importance Methods for Scientific Inference\n\nIn these exercises you will:\n\n1. Discover **why Permutation Feature Importance (PFI) can be misleading** when features are correlated \u2014 and implement PFI yourself\n2. Compute **Conditional Feature Importance (CFI)** using the `fippy` package\n3. Compute **Leave-One-Covariate-Out (LOCO)** importance and interpret it in terms of explained variance\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\nRun all cells in this section before starting the exercises."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install fippy -q"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'font.size':        14,\n    'axes.titlesize':   16,\n    'axes.labelsize':   14,\n    'xtick.labelsize':  13,\n    'ytick.labelsize':  13,\n})"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Data\n\nWe generate a dataset with 5 features and a continuous target variable $Y$.\n**The data-generating process is hidden for now.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_data(n=1500, seed=83):\n    rng = np.random.RandomState(seed)\n    x1 = rng.normal(0, 1, n)\n    x2 = 0.999 * x1 + np.sqrt(1 - 0.999**2) * rng.normal(0, 1, n)\n    x3 = rng.normal(0, 1, n)\n    x4 = 0.999 * x3 + np.sqrt(1 - 0.999**2) * rng.normal(0, 1, n)\n    y  = 5 * x1 + rng.normal(0, 1, n)\n    x5 = rng.normal(0, 1, n)\n    X  = np.column_stack([x1, x2, x3, x4, x5])\n    return X, y\n\nX, y = generate_data()\nfeature_names = [\"X1\", \"X2\", \"X3\", \"X4\", \"X5\"]\nprint(f\"Dataset: {X.shape}, features: {feature_names}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Model\n\nWe train an OLS linear regression on 1000 training observations and evaluate on 500 held-out test observations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "n_train = 1000\nX_train, X_test = X[:n_train], X[n_train:]\ny_train, y_test = y[:n_train], y[n_train:]\n\nmodel = LinearRegression().fit(X_train, y_train)\nr2  = model.score(X_test, y_test)\nprint(f\"Test R\\u00b2: {r2:.3f}\")\nprint(f\"Coefficients: {np.round(model.coef_, 2)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### fippy setup\n\nWe use the [`fippy`](https://github.com/gcskoenig/fippy) package for Exercises 2 and 3.\nThe **Gaussian sampler** estimates the conditional distribution $P(X_j \\mid X_{-j})$ in closed form\nusing the multivariate normal assumption."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from fippy.explainers import Explainer\nfrom fippy.samplers import GaussianSampler\n\n# fippy requires pandas DataFrames\nX_train_df = pd.DataFrame(X_train, columns=feature_names)\nX_test_df  = pd.DataFrame(X_test,  columns=feature_names)\ny_train_s  = pd.Series(y_train, name='y')\ny_test_s   = pd.Series(y_test,  name='y')\n\nsampler   = GaussianSampler(X_train_df)\nexplainer = Explainer(model.predict, X_train_df,\n                      loss=mean_squared_error, sampler=sampler)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Exercise 1: Implement PFI and Understand Why It Can Mislead\n\nNow we reveal the true data-generating process:\n\n$$X_1 \\sim \\mathcal{N}(0,1), \\quad X_3 \\sim \\mathcal{N}(0,1), \\quad X_5 \\sim \\mathcal{N}(0,1) \\quad \\text{(mutually independent)}$$\n$$X_2 = 0.999\\,X_1 + \\varepsilon_2, \\qquad X_4 = 0.999\\,X_3 + \\varepsilon_4$$\n$$Y = 5\\,X_1 + \\varepsilon_Y \\qquad \\Rightarrow \\text{only } X_1 \\text{ causes } Y$$\n\nThe fitted model is $\\hat{f}(X) = 3.11\\,X_1 + 1.88\\,X_2 - 2.11\\,X_3 + 2.17\\,X_4 + 0.02\\,X_5$.\n\n**Tasks**\n\n1. Complete `my_pfi` below \u2014 the only missing piece is the permutation of one column.\n2. Create scatterplots of $(X_3, X_4)$ before and after permuting $X_3$.\n3. Explain why PFI assigns high importance to $X_3$ and $X_4$ even though they are independent of $Y$."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def my_pfi(model, X, y, feature_idx, n_repeats=50, seed=42):\n    \"\"\"\n    Permutation Feature Importance for a single feature.\n\n    PFI_j = mean_r [ L(y, f(X_perm_r)) ] - L(y, f(X))\n\n    where X_perm_r is X with column feature_idx randomly permuted.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    baseline_mse = mean_squared_error(y, model.predict(X))\n\n    perturbed_mses = []\n    for _ in range(n_repeats):\n        X_perm = X.copy()\n        # \u2500\u2500 YOUR CODE HERE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        # Permute column feature_idx of X_perm using rng.permutation()\n\n\n        # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        perturbed_mses.append(mean_squared_error(y, model.predict(X_perm)))\n\n    return np.mean(perturbed_mses) - baseline_mse\n\n\n# Compute and plot PFI for all features\npfi_scores = [my_pfi(model, X_test, y_test, j) for j in range(len(feature_names))]\nfor name, score in zip(feature_names, pfi_scores):\n    print(f\"PFI({name}): {score:.4f}\")\n\nplt.figure(figsize=(6, 4))\nplt.barh(feature_names[::-1], pfi_scores[::-1], color='grey', edgecolor='black', linewidth=0.5)\nplt.xlabel(\"PFI (increase in MSE)\")\nplt.title(\"Permutation Feature Importance\")\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Scatterplot: X3 vs X4 before and after permuting X3\nrng = np.random.RandomState(42)\nX_perm = X_test.copy()\n# \u2500\u2500 YOUR CODE HERE: permute column 2 (X3) of X_perm \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# \u2500\u2500 YOUR CODE HERE: scatter X_test[:,2] vs X_test[:,3] on axes[0] \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\naxes[0].set(xlabel=\"$X_3$\", ylabel=\"$X_4$\", title=\"Original: $(X_3, X_4)$\",\n            xlim=(-4,4), ylim=(-4,4))\n\n# \u2500\u2500 YOUR CODE HERE: scatter X_perm[:,2] vs X_perm[:,3] on axes[1] \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\naxes[1].set(xlabel=r\"$\\tilde{X}_3$ (permuted)\", ylabel=\"$X_4$\",\n            title=\"After permuting $X_3$\", xlim=(-4,4), ylim=(-4,4))\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Your explanation:**\n\n- What do you notice in the scatterplots?\n  - *Your answer here*\n\n- Why does PFI assign high importance to $X_3$ and $X_4$?\n  - *Your answer here*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Exercise 2: Conditional Feature Importance (CFI)\n\nInstead of breaking all dependencies (permutation sampler), CFI resamples $X_j$ from\n$P(X_j \\mid X_{-j})$, preserving the feature correlations.\n\n$$\\text{CFI}_j = \\mathbb{E}[L(Y, \\hat{f}(\\tilde{X}_j, X_{-j}))] - \\mathbb{E}[L(Y, \\hat{f}(X))], \\quad \\tilde{X}_j \\sim P(X_j \\mid X_{-j})$$\n\nWe use `fippy`'s `GaussianSampler` to estimate the conditional distribution in closed form."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Conditional Feature Importance via fippy\nex_cfi = explainer.cfi(X_test_df, y_test_s, nr_runs=10)\nex_cfi.hbarplot()\nplt.show()\n\nmeans, stds = ex_cfi.fi_means_stds()\nprint(\"\\nCFI scores:\")\nfor feat, m, s in zip(feature_names, means, stds):\n    print(f\"  {feat}: {m:.4f} \u00b1 {s:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Interpretation:**\n\n- How do CFI scores differ from PFI? Why?\n  - *Your answer here*\n\n- Which features receive non-zero CFI, and why?\n  - *Your answer here*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Exercise 3: Leave-One-Covariate-Out (LOCO)\n\nLOCO measures how much the model's **explained variance** drops when a feature is removed entirely.\n\nUsing the conditional SAGE value function $v(S)$ \u2014 which quantifies the explained variance\nwhen only features $S$ are available:\n\n$$v(S) = \\mathbb{E}[(Y - \\mathbb{E}[f(X)])^2] - \\mathbb{E}[(Y - \\mathbb{E}[f(X)\\mid X_S])^2]$$\n\nLOCO is:\n$$\\text{LOCO}_j = v(\\{1,\\ldots,p\\}) - v(\\{1,\\ldots,p\\} \\setminus \\{j\\})$$\n\nWe compute $v(S)$ using `fippy`'s `csagevf` function and the Gaussian sampler."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute v(S) for all features and for each leave-one-out set\nN = feature_names  # full coalition\n\nex_vN = explainer.csagevf(S=list(N), X_eval=X_test_df, y_eval=y_test_s)\nmeans_N, _ = ex_vN.fi_means_stds()\nv_N = float(np.array(means_N).flatten()[0])\nprint(f\"v(all features) = {v_N:.4f}  [\u2248 Var(Y)\u00b7R\u00b2]\\n\")\n\nloco_scores = {}\nfor feat in N:\n    N_minus_j = [f for f in N if f != feat]\n    ex_vNj = explainer.csagevf(S=N_minus_j, X_eval=X_test_df, y_eval=y_test_s)\n    means_Nj, _ = ex_vNj.fi_means_stds()\n    v_Nj = float(np.array(means_Nj).flatten()[0])\n    loco_scores[feat] = v_N - v_Nj\n    print(f\"LOCO({feat}): {loco_scores[feat]:.4f}  \"\n          f\"({100 * loco_scores[feat] / v_N:.1f}% of explained variance)\")\n\n# Plot as share of explained variance\npct   = [100 * loco_scores[f] / v_N for f in N]\nplt.figure(figsize=(6, 4))\nplt.barh(N[::-1], pct[::-1], color='grey', edgecolor='black', linewidth=0.5)\nplt.xlabel(\"Share of explained variance (%)\")\nplt.title(\"LOCO (conditional marginalization)\")\nplt.axvline(0, color='black', linewidth=0.8)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Interpretation:**\n\n- Which features have non-zero LOCO, and what fraction of $R^2$ do they explain?\n  - *Your answer here*\n\n- How do LOCO scores differ from CFI, and what additional information do they provide?\n  - *Your answer here*"
  }
 ]
}