{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Methods for Scientific Inference\n",
    "\n",
    "In these exercises you will:\n",
    "\n",
    "1. Compute and interpret **Permutation Feature Importance (PFI)** scores\n",
    "2. Discover **why PFI can be misleading** when features are correlated\n",
    "3. Use a **conditional sampler** to compute Conditional Feature Importance (CFI) and compare\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the cells below to set everything up. **You do not need to modify any code in this section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "\n",
    "We generate a dataset with 4 features and a continuous target variable $Y$.\n",
    "\n",
    "**You do not know the data generating process (DGP) yet.** You only observe the features $X_1, X_2, X_3, X_4$ and the target $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n=1500, seed=83):\n",
    "    \"\"\"Generate the dataset. The DGP is hidden for now.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    x1 = rng.normal(0, 1, n)\n",
    "    x2 = 0.999 * x1 + np.sqrt(1 - 0.999**2) * rng.normal(0, 1, n)\n",
    "    x3 = rng.normal(0, 1, n)\n",
    "    x4 = 0.999 * x3 + np.sqrt(1 - 0.999**2) * rng.normal(0, 1, n)\n",
    "    y = 5 * x1 + rng.normal(0, 1, n)\n",
    "    X = np.column_stack([x1, x2, x3, x4])\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_data()\n",
    "feature_names = [\"X1\", \"X2\", \"X3\", \"X4\"]\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Feature names: {feature_names}\")\n",
    "print(f\"\\nFirst 5 rows of X:\")\n",
    "print(np.round(X[:5], 2))\n",
    "print(f\"\\nFirst 5 values of Y: {np.round(y[:5], 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model\n",
    "\n",
    "We train a Linear Regression on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "n_train = 1000\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "# Train a Linear Regression (unregularized)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = model.score(X_test, y_test)\n",
    "print(f\"Test MSE: {mse:.3f}\")\n",
    "print(f\"Test R\\u00b2:  {r2:.3f}\")\n",
    "print(f\"\\nFitted coefficients: {np.round(model.coef_, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provided functions: Permutation Sampler and PFI\n",
    "\n",
    "Below we provide:\n",
    "- A **permutation sampler**: for a given feature $j$, it randomly shuffles (permutes) the values of $X_j$ across observations, breaking the association between $X_j$ and all other variables.\n",
    "- A **PFI function**: computes the Permutation Feature Importance:\n",
    "\n",
    "$$\\text{PFI}_j = \\mathbb{E}[L(Y, \\hat{f}(\\tilde{X}_j, X_{-j}))] - \\mathbb{E}[L(Y, \\hat{f}(X))]$$\n",
    "\n",
    "where $\\tilde{X}_j$ is drawn independently from the marginal distribution of $X_j$ (i.e., permuted), and $L$ is the loss function (here: MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_sampler(X, feature_idx, rng=None):\n",
    "    \"\"\"\n",
    "    Permutation (marginal) sampler.\n",
    "    Returns a copy of X where column `feature_idx` is randomly permuted,\n",
    "    effectively sampling X_j from its marginal distribution independently\n",
    "    of all other features and Y.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.RandomState(0)\n",
    "    X_perm = X.copy()\n",
    "    X_perm[:, feature_idx] = rng.permutation(X[:, feature_idx])\n",
    "    return X_perm\n",
    "\n",
    "\n",
    "def compute_pfi(model, X, y, feature_idx, sampler, n_repeats=25, seed=42):\n",
    "    \"\"\"\n",
    "    Compute Permutation Feature Importance for feature `feature_idx`.\n",
    "\n",
    "    PFI_j = E[L(Y, f(X_tilde_j, X_{-j}))] - E[L(Y, f(X))]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : fitted sklearn model\n",
    "    X : np.ndarray, shape (n, p) - test features\n",
    "    y : np.ndarray, shape (n,) - test target\n",
    "    feature_idx : int - index of the feature to permute\n",
    "    sampler : callable - function(X, feature_idx, rng) -> X_perturbed\n",
    "    n_repeats : int - number of repetitions to average over\n",
    "    seed : int - random seed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pfi : float - the PFI score\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    baseline_mse = mean_squared_error(y, model.predict(X))\n",
    "\n",
    "    perturbed_mses = []\n",
    "    for _ in range(n_repeats):\n",
    "        X_perturbed = sampler(X, feature_idx, rng=rng)\n",
    "        y_pred_perturbed = model.predict(X_perturbed)\n",
    "        perturbed_mses.append(mean_squared_error(y, y_pred_perturbed))\n",
    "\n",
    "    pfi = np.mean(perturbed_mses) - baseline_mse\n",
    "    return pfi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 1: Compute and Interpret PFI\n",
    "\n",
    "Use the provided `compute_pfi` function with the `permutation_sampler` to compute PFI for all four features.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Compute PFI for each feature ($X_1, X_2, X_3, X_4$) and display the results as a bar chart.\n",
    "2. Answer the following questions:\n",
    "   - Which features does the **model** rely on for its predictions?\n",
    "   - Based on PFI alone, which features would you conclude are **important in the data** (i.e., associated with $Y$)?\n",
    "   - What exactly does PFI measure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Step 1: Compute PFI for all features\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Create a bar chart of PFI values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your interpretation (double-click to edit):**\n",
    "\n",
    "- Which features does the model rely on?\n",
    "  - *Your answer here*\n",
    "\n",
    "- Which features appear important in the data?\n",
    "  - *Your answer here*\n",
    "\n",
    "- What does PFI measure exactly?\n",
    "  - *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 2: Why is PFI misleading here?\n",
    "\n",
    "Now we reveal the **true data generating process (DGP)**:\n",
    "\n",
    "$$X_1 \\sim \\mathcal{N}(0,1), \\quad X_3 \\sim \\mathcal{N}(0,1) \\quad \\text{(independent of each other and of } X_1 \\text{)}$$\n",
    "\n",
    "$$X_2 = 0.999 \\cdot X_1 + \\sqrt{1 - 0.999^2} \\cdot \\varepsilon_2, \\quad \\varepsilon_2 \\sim \\mathcal{N}(0,1)$$\n",
    "\n",
    "$$X_4 = 0.999 \\cdot X_3 + \\sqrt{1 - 0.999^2} \\cdot \\varepsilon_4, \\quad \\varepsilon_4 \\sim \\mathcal{N}(0,1)$$\n",
    "\n",
    "$$Y = 5 X_1 + \\varepsilon_Y, \\quad \\varepsilon_Y \\sim \\mathcal{N}(0, 1)$$\n",
    "\n",
    "**Key observations about the DGP:**\n",
    "- $Y$ depends **only** on $X_1$\n",
    "- $X_2$ is **not a direct cause** of $Y$; it is only correlated with $Y$ through $X_1$\n",
    "- $X_3$ and $X_4$ are **completely independent** of $Y$; they are only correlated with each other\n",
    "\n",
    "Yet, PFI assigns substantial importance scores to **all four features**!\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Create scatterplots comparing the **original** data vs. the **permuted** data for the pairs $(X_1, X_2)$ and $(X_3, X_4)$. What do you notice?\n",
    "2. Look at the fitted model coefficients (printed in the setup). How do they relate to the PFI scores?\n",
    "3. Explain in your own words **why** PFI gives $X_3$ and $X_4$ high importance scores, even though they are completely independent of $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Step 1: Create permuted versions of the test data\n",
    "#   - Permute X2 (feature index 1)\n",
    "#   - Permute X3 (feature index 2)\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Create side-by-side scatterplots\n",
    "#   - (X1, X2) in original vs. permuted data\n",
    "#   - (X3, X4) in original vs. permuted data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your explanation (double-click to edit):**\n",
    "\n",
    "- What do you notice in the scatterplots?\n",
    "  - *Your answer here*\n",
    "\n",
    "- How do the model coefficients explain the PFI scores?\n",
    "  - *Your answer here*\n",
    "\n",
    "- Why does PFI assign high importance to $X_3$ and $X_4$?\n",
    "  - *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 3: Conditional Feature Importance (CFI)\n",
    "\n",
    "To address the problem from Exercise 2, we can use a **conditional sampler** instead of the permutation sampler.\n",
    "\n",
    "Instead of sampling $\\tilde{X}_j$ from the marginal distribution (breaking all dependencies), we sample from the **conditional distribution**:\n",
    "\n",
    "$$\\tilde{X}_j \\sim F_{X_j | X_{-j}}$$\n",
    "\n",
    "This preserves the dependencies between features while still breaking the direct association between $X_j$ and $Y$.\n",
    "\n",
    "For multivariate normal data, the conditional distribution has a closed-form solution:\n",
    "\n",
    "$$X_j \\mid X_{-j} = x_{-j} \\sim \\mathcal{N}\\left(\\mu_{j|-j},\\; \\sigma^2_{j|-j}\\right)$$\n",
    "\n",
    "where:\n",
    "- $\\mu_{j|-j} = \\mu_j + \\Sigma_{j,-j} \\Sigma_{-j,-j}^{-1}(x_{-j} - \\mu_{-j})$\n",
    "- $\\sigma^2_{j|-j} = \\Sigma_{j,j} - \\Sigma_{j,-j} \\Sigma_{-j,-j}^{-1} \\Sigma_{-j,j}$\n",
    "\n",
    "The conditional sampler is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_sampler(X, feature_idx, rng=None, mean=None, cov=None):\n",
    "    \"\"\"\n",
    "    Conditional sampler for multivariate normal data.\n",
    "    Samples X_j from X_j | X_{-j} using the closed-form Gaussian conditional.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (n, p)\n",
    "    feature_idx : int - the feature to resample\n",
    "    rng : np.random.RandomState\n",
    "    mean : np.ndarray, shape (p,) - mean of the joint distribution\n",
    "    cov : np.ndarray, shape (p, p) - covariance of the joint distribution\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_cond : np.ndarray - copy of X with column `feature_idx` resampled conditionally\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.RandomState(0)\n",
    "\n",
    "    n, p = X.shape\n",
    "    j = feature_idx\n",
    "\n",
    "    # Indices of all other features\n",
    "    others = [i for i in range(p) if i != j]\n",
    "\n",
    "    # Extract sub-matrices from the covariance\n",
    "    sigma_jj = cov[j, j]                              # scalar\n",
    "    sigma_j_others = cov[j, others]                    # (p-1,)\n",
    "    sigma_others_others = cov[np.ix_(others, others)]  # (p-1, p-1)\n",
    "\n",
    "    # Conditional parameters\n",
    "    sigma_others_inv = np.linalg.inv(sigma_others_others)\n",
    "    beta = sigma_j_others @ sigma_others_inv            # regression coefficients\n",
    "    cond_var = sigma_jj - sigma_j_others @ sigma_others_inv @ sigma_j_others  # conditional variance\n",
    "\n",
    "    # Conditional mean for each observation: mu_j + beta @ (x_{-j} - mu_{-j})\n",
    "    x_others = X[:, others]\n",
    "    cond_means = mean[j] + (x_others - mean[others]) @ beta  # (n,)\n",
    "\n",
    "    # Sample from conditional distribution\n",
    "    X_cond = X.copy()\n",
    "    X_cond[:, j] = cond_means + rng.normal(0, np.sqrt(max(cond_var, 0)), n)\n",
    "\n",
    "    return X_cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the conditional sampler with `compute_pfi`, we need to wrap it so it has the same signature. We estimate the mean and covariance from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate mean and covariance from training data\n",
    "estimated_mean = np.mean(X_train, axis=0)\n",
    "estimated_cov = np.cov(X_train, rowvar=False)\n",
    "\n",
    "print(\"Estimated covariance matrix:\")\n",
    "print(np.round(estimated_cov, 3))\n",
    "print(\"\\nNotice: X1-X2 are highly correlated, X3-X4 are highly correlated,\")\n",
    "print(\"but the two pairs are independent of each other.\")\n",
    "\n",
    "\n",
    "def conditional_sampler_wrapper(X, feature_idx, rng=None):\n",
    "    \"\"\"Wrapper so conditional_sampler has the same signature as permutation_sampler.\"\"\"\n",
    "    return conditional_sampler(X, feature_idx, rng=rng,\n",
    "                               mean=estimated_mean, cov=estimated_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks:**\n",
    "\n",
    "1. Compute **Conditional Feature Importance (CFI)** for all features using `compute_pfi` with the `conditional_sampler_wrapper`.\n",
    "2. Create a side-by-side bar chart comparing PFI and CFI for all features.\n",
    "3. (Optional) Create scatterplots of $(X_1, X_2)$ and $(X_3, X_4)$ after conditional resampling and compare to the permutation scatterplots from Exercise 2.\n",
    "4. Interpret the results:\n",
    "   - How do PFI and CFI differ? Why?\n",
    "   - Recall the three types of features in the DGP: $X_1$ (directly relevant), $X_2$ (indirectly relevant), $X_3$/$X_4$ (completely irrelevant). How does CFI treat each of these?\n",
    "   - What does this mean for drawing scientific conclusions from feature importance scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Step 1: Compute CFI for all features\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Create a side-by-side comparison bar chart (PFI vs CFI)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (optional)\n",
    "# Step 3: Scatterplots of (X1, X2) and (X3, X4) after conditional resampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your interpretation (double-click to edit):**\n",
    "\n",
    "- How do PFI and CFI differ? Why?\n",
    "  - *Your answer here*\n",
    "\n",
    "- How does CFI treat each type of feature?\n",
    "  - *Your answer here*\n",
    "\n",
    "- What does this mean for scientific inference?\n",
    "  - *Your answer here*"
   ]
  }
 ]
}