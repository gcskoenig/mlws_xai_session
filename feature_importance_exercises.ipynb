{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Feature Importance Methods for Scientific Inference\n\nIn these exercises you will:\n\n1. Interpret a **Permutation Feature Importance (PFI)** plot and reflect on what PFI measures\n2. Discover **why PFI can be misleading** when features are correlated \u2014 and implement PFI yourself\n3. Use a **conditional sampler** to compute **Conditional Feature Importance (CFI)** and compare\n4. Implement **Leave-One-Covariate-Out (LOCO)** importance based on conditional marginalization\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\nRun the cells below to set everything up. **You do not need to modify any code in this section.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'font.size':        14,\n    'axes.titlesize':   16,\n    'axes.labelsize':   14,\n    'xtick.labelsize':  13,\n    'ytick.labelsize':  13,\n    'legend.fontsize':  13,\n})"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Data Generation\n\nWe generate a dataset with 5 features and a continuous target variable $Y$.\n\n**You do not know the data generating process (DGP) yet.** You only observe the features $X_1, X_2, X_3, X_4, X_5$ and the target $Y$."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_data(n=1500, seed=83):\n    \"\"\"Generate the dataset. The DGP is hidden for now.\"\"\"\n    rng = np.random.RandomState(seed)\n    x1 = rng.normal(0, 1, n)\n    x2 = 0.999 * x1 + np.sqrt(1 - 0.999**2) * rng.normal(0, 1, n)\n    x3 = rng.normal(0, 1, n)\n    x4 = 0.999 * x3 + np.sqrt(1 - 0.999**2) * rng.normal(0, 1, n)\n    y = 5 * x1 + rng.normal(0, 1, n)\n    x5 = rng.normal(0, 1, n)\n    X = np.column_stack([x1, x2, x3, x4, x5])\n    return X, y\n\nX, y = generate_data()\nfeature_names = [\"X1\", \"X2\", \"X3\", \"X4\", \"X5\"]\n\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Feature names: {feature_names}\")\nprint(f\"\\nFirst 5 rows of X:\")\nprint(np.round(X[:5], 2))\nprint(f\"\\nFirst 5 values of Y: {np.round(y[:5], 2)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Train a model\n\nWe train a Linear Regression on the data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split into train and test\nn_train = 1000\nX_train, X_test = X[:n_train], X[n_train:]\ny_train, y_test = y[:n_train], y[n_train:]\n\n# Train a Linear Regression (unregularized)\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Evaluate\ny_pred = model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nr2 = model.score(X_test, y_test)\nprint(f\"Test MSE: {mse:.3f}\")\nprint(f\"Test R\\u00b2:  {r2:.3f}\")\nprint(f\"\\nFitted coefficients: {np.round(model.coef_, 2)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Provided: Permutation Sampler and PFI\n\nBelow we provide:\n- A **permutation sampler** that randomly shuffles $X_j$, breaking its association with all other variables.\n- A **`compute_pfi` function** that computes PFI by averaging the loss increase over many permutations.\n\n$$\\text{PFI}_j = \\mathbb{E}[L(Y, \\hat{f}(\\tilde{X}_j, X_{-j}))] - \\mathbb{E}[L(Y, \\hat{f}(X))]$$\n\nwhere $\\tilde{X}_j \\sim P(X_j)$ is drawn independently of everything else."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def permutation_sampler(X, feature_idx, rng=None):\n    \"\"\"\n    Permutation (marginal) sampler.\n    Returns a copy of X where column `feature_idx` is randomly permuted.\n    \"\"\"\n    if rng is None:\n        rng = np.random.RandomState(0)\n    X_perm = X.copy()\n    X_perm[:, feature_idx] = rng.permutation(X[:, feature_idx])\n    return X_perm\n\n\ndef compute_pfi(model, X, y, feature_idx, sampler, n_repeats=50, seed=42):\n    \"\"\"\n    Compute Permutation Feature Importance for feature `feature_idx`.\n    Returns the mean increase in MSE over n_repeats permutations.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    baseline_mse = mean_squared_error(y, model.predict(X))\n    perturbed_mses = []\n    for _ in range(n_repeats):\n        X_perturbed = sampler(X, feature_idx, rng=rng)\n        perturbed_mses.append(mean_squared_error(y, model.predict(X_perturbed)))\n    return np.mean(perturbed_mses) - baseline_mse"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Provided: PFI Results\n\nThe cell below computes PFI for all features and shows the bar chart. **You do not need to modify this cell.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Pre-computed PFI results (provided)\npfi_scores = [compute_pfi(model, X_test, y_test, feature_idx=j,\n                          sampler=permutation_sampler)\n              for j in range(X_test.shape[1])]\n\nplt.figure(figsize=(6, 4))\nplt.barh(feature_names[::-1], pfi_scores[::-1],\n         color='grey', edgecolor='black', linewidth=0.5)\nplt.xlabel(\"PFI (increase in MSE)\")\nplt.title(\"Permutation Feature Importance\")\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Exercise 1: Interpret the PFI Plot\n\nLook at the PFI bar chart above. The model achieves $R^2 = 0.963$ on the test set.\n\n**Questions:**\n\n1. Which features does the **model** rely on for its predictions?\n2. Based on PFI alone, which features would you conclude are **important in the data** (i.e., associated with $Y$)?\n3. What exactly does PFI measure?"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Your interpretation (double-click to edit):**\n\n- Which features does the model rely on?\n  - *Your answer here*\n\n- Which features appear important in the data?\n  - *Your answer here*\n\n- What does PFI measure exactly?\n  - *Your answer here*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Exercise 2: Why is PFI Misleading? Implement it Yourself.\n\nNow we reveal the **true data generating process (DGP)**:\n\n$$X_1 \\sim \\mathcal{N}(0,1), \\quad X_3 \\sim \\mathcal{N}(0,1), \\quad X_5 \\sim \\mathcal{N}(0,1) \\quad \\text{(mutually independent)}$$\n\n$$X_2 = 0.999 \\cdot X_1 + \\sqrt{1 - 0.999^2} \\cdot \\varepsilon_2, \\quad X_4 = 0.999 \\cdot X_3 + \\sqrt{1 - 0.999^2} \\cdot \\varepsilon_4$$\n\n$$Y = 5 X_1 + \\varepsilon_Y, \\quad \\varepsilon_Y \\sim \\mathcal{N}(0, 1)$$\n\nThe fitted model is:\n$$\\hat{f}(X) = 3.11\\,X_1 + 1.88\\,X_2 - 2.11\\,X_3 + 2.17\\,X_4 + 0.02\\,X_5$$\n\nWere your interpretations from Exercise 1 correct?\n\n**Tasks:**\n\n1. **Implement PFI from scratch.** Write `my_pfi(model, X, y, feature_idx)` that permutes feature `feature_idx` once and measures the MSE increase. Average over 50 repetitions. Verify it matches the provided `compute_pfi`.\n2. **Create scatterplots** of $(X_3, X_4)$ before and after permuting $X_3$. What do you notice?\n3. **Explain** why PFI assigns high importance to $X_3$ and $X_4$ even though they are completely independent of $Y$."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Task 1: Implement PFI from scratch\ndef my_pfi(model, X, y, feature_idx, n_repeats=50, seed=42):\n    \"\"\"\n    Compute PFI for a single feature by permuting it n_repeats times.\n    Returns the mean increase in MSE.\n    \"\"\"\n    pass  # replace with your implementation\n\n\n# Verify against the provided compute_pfi\nfor j in range(X_test.shape[1]):\n    mine     = my_pfi(model, X_test, y_test, feature_idx=j)\n    provided = pfi_scores[j]\n    print(f\"X{j+1}: mine={mine:.4f}  provided={provided:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Task 2: Scatterplots of (X3, X4) before and after permuting X3\nrng = np.random.RandomState(42)\nX_test_perm_x3 = permutation_sampler(X_test, feature_idx=2, rng=rng)\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\naxes[0].scatter(X_test[:, 2], X_test[:, 3], alpha=0.3, s=10, color='grey')\naxes[0].set_xlabel(\"$X_3$\"); axes[0].set_ylabel(\"$X_4$\")\naxes[0].set_title(\"Original: $(X_3, X_4)$\")\naxes[0].set_xlim(-4, 4); axes[0].set_ylim(-4, 4)\n\naxes[1].scatter(X_test_perm_x3[:, 2], X_test_perm_x3[:, 3], alpha=0.3, s=10, color='grey')\naxes[1].set_xlabel(r\"$\\tilde{X}_3$ (permuted)\"); axes[1].set_ylabel(\"$X_4$\")\naxes[1].set_title(\"After permuting $X_3$\")\naxes[1].set_xlim(-4, 4); axes[1].set_ylim(-4, 4)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Your explanation (double-click to edit):**\n\n- What do you notice in the scatterplots?\n  - *Your answer here*\n\n- How do the model coefficients explain the PFI scores?\n  - *Your answer here*\n\n- Why does PFI assign high importance to $X_3$ and $X_4$?\n  - *Your answer here*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Exercise 3: Conditional Feature Importance (CFI)\n\nTo address the problem from Exercise 2, we use a **conditional sampler** instead of the permutation sampler.\n\nInstead of sampling $\\tilde{X}_j$ from the marginal (breaking all dependencies), we sample from:\n\n$$\\tilde{X}_j \\sim P(X_j \\mid X_{-j})$$\n\nThis preserves the dependencies between features while still breaking the direct $X_j$\u2013$Y$ link.\n\nFor multivariate normal data this has a closed form:\n\n$$X_j \\mid X_{-j} = x_{-j} \\sim \\mathcal{N}\\!\\left(\\mu_j + \\Sigma_{j,-j}\\,\\Sigma_{-j,-j}^{-1}(x_{-j} - \\mu_{-j}),\\; \\Sigma_{jj} - \\Sigma_{j,-j}\\,\\Sigma_{-j,-j}^{-1}\\,\\Sigma_{-j,j}\\right)$$\n\nThe conditional sampler and wrapper are provided below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def conditional_sampler(X, feature_idx, rng=None, mean=None, cov=None):\n    \"\"\"\n    Conditional sampler for multivariate normal data.\n    Samples X_j from X_j | X_{-j} using the closed-form Gaussian conditional.\n    \"\"\"\n    if rng is None:\n        rng = np.random.RandomState(0)\n    n, p = X.shape\n    j = feature_idx\n    others = [i for i in range(p) if i != j]\n\n    sigma_jj           = cov[j, j]\n    sigma_j_others     = cov[j, others]\n    sigma_others_others = cov[np.ix_(others, others)]\n    sigma_others_inv   = np.linalg.inv(sigma_others_others)\n    beta               = sigma_j_others @ sigma_others_inv\n    cond_var           = sigma_jj - sigma_j_others @ sigma_others_inv @ sigma_j_others\n\n    x_others   = X[:, others]\n    cond_means = mean[j] + (x_others - mean[others]) @ beta\n    X_cond     = X.copy()\n    X_cond[:, j] = cond_means + rng.normal(0, np.sqrt(max(cond_var, 0)), n)\n    return X_cond\n\n\n# Estimate mean and covariance from training data\nestimated_mean = np.mean(X_train, axis=0)\nestimated_cov  = np.cov(X_train, rowvar=False)\n\nprint(\"Estimated covariance matrix:\")\nprint(np.round(estimated_cov, 3))\n\ndef conditional_sampler_wrapper(X, feature_idx, rng=None):\n    return conditional_sampler(X, feature_idx, rng=rng,\n                               mean=estimated_mean, cov=estimated_cov)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Tasks:**\n\n1. Compute **CFI** for all features using `compute_pfi` with the `conditional_sampler_wrapper`.\n2. Create a side-by-side bar chart comparing PFI and CFI.\n3. Interpret the results:\n   - How do PFI and CFI differ? Why?\n   - How does CFI treat each type of feature ($X_1$ directly relevant, $X_2$ indirectly relevant, $X_3$/$X_4$ irrelevant-collinear, $X_5$ purely irrelevant)?\n   - What does this mean for scientific inference?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Task 1: Compute CFI for all features\ncfi_scores = []\nfor j in range(X_test.shape[1]):\n    cfi_scores.append(compute_pfi(model, X_test, y_test, feature_idx=j,\n                                  sampler=conditional_sampler_wrapper))\n\n# Task 2: Side-by-side bar chart\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\nnames_r = feature_names[::-1]\n\naxes[0].barh(names_r, pfi_scores[::-1], color='grey', edgecolor='black', linewidth=0.5)\naxes[0].set_xlabel(\"Importance (increase in MSE)\")\naxes[0].set_title(\"PFI (Permutation / Marginal Sampler)\")\n\naxes[1].barh(names_r, cfi_scores[::-1], color='grey', edgecolor='black', linewidth=0.5)\naxes[1].set_xlabel(\"Importance (increase in MSE)\")\naxes[1].set_title(\"CFI (Conditional Sampler)\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Your interpretation (double-click to edit):**\n\n- How do PFI and CFI differ? Why?\n  - *Your answer here*\n\n- How does CFI treat each type of feature?\n  - *Your answer here*\n\n- What does this mean for scientific inference?\n  - *Your answer here*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Exercise 4: Leave-One-Covariate-Out (LOCO)\n\nCFI asks: *how much does performance drop when we remove the association between $X_j$ and $Y$?*\n**LOCO** asks: *how much does performance drop when we remove feature $j$ from the model entirely?*\n\n## The conditional SAGE value function\n\nFor a coalition (subset) $S \\subseteq \\{1,\\ldots,p\\}$, define:\n\n$$v(S) = \\underbrace{\\mathbb{E}\\!\\left[(Y - \\mathbb{E}[f(X)])^2\\right]}_{\\text{baseline MSE}} - \\underbrace{\\mathbb{E}\\!\\left[(Y - \\mathbb{E}[f(X)\\mid X_S])^2\\right]}_{\\text{MSE using only } X_S}$$\n\n$v(S)$ measures how much explained variance is gained by using only the features in $S$.\nNote $v(\\emptyset) = 0$ and $v(\\{1,\\ldots,p\\}) \\approx \\text{Var}(Y) \\cdot R^2$.\n\nFor a **linear model** $f(X) = \\beta_0 + \\beta^\\top X$ with **Gaussian features**:\n\n$$\\mathbb{E}[f(X) \\mid X_S] = \\beta_0 + \\beta_S^\\top X_S + \\beta_{S^c}^\\top\\!\\left(\\mu_{S^c} + \\Sigma_{S^c,S}\\,\\Sigma_{S,S}^{-1}(X_S - \\mu_S)\\right)$$\n\n## LOCO from the value function\n\n$$\\text{LOCO}_j = v(\\{1,\\ldots,p\\}) - v(\\{1,\\ldots,p\\} \\setminus \\{j\\})$$\n\nThis is the drop in explained variance when feature $j$ is removed from the full set.\n\n---\n\n**Tasks:**\n\n1. Implement `conditional_sage_value(S, model, X, y, mean, cov)` that computes $v(S)$ using the closed-form Gaussian conditional for a linear model.\n2. Compute **LOCO** for all features as $v(N) - v(N \\setminus \\{j\\})$.\n3. Normalise by the baseline MSE to express each score as a **share of explained variance (%)**.\n4. Plot the results (horizontal grey bars, $X_1$ on top) and interpret."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Task 1: Implement the conditional SAGE value function\ndef conditional_sage_value(S, model, X, y, mean, cov):\n    \"\"\"\n    Compute v(S) = MSE_base - E[(Y - E[f(X)|X_S])^2].\n\n    For a linear model with Gaussian features, E[f(X)|X_S] is:\n      beta0 + beta_S @ X_S + beta_Sc @ (mean_Sc + Sigma_{Sc,S} @ inv(Sigma_{S,S}) @ (X_S - mean_S))\n\n    Parameters\n    ----------\n    S    : tuple of int  \u2013 feature indices in the coalition\n    model: fitted LinearRegression\n    X    : np.ndarray (n, p)\n    y    : np.ndarray (n,)\n    mean : np.ndarray (p,)  \u2013 estimated feature means\n    cov  : np.ndarray (p,p) \u2013 estimated feature covariance\n\n    Returns\n    -------\n    float \u2013 v(S)\n    \"\"\"\n    pass  # replace with your implementation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Task 2 & 3: Compute LOCO scores and normalise\np = X_test.shape[1]\nN = tuple(range(p))\n\nloco_scores_ex4 = []\nfor j in range(p):\n    N_minus_j = tuple(i for i in range(p) if i != j)\n    loco_j = conditional_sage_value(N, model, X_test, y_test, estimated_mean, estimated_cov) \\\n           - conditional_sage_value(N_minus_j, model, X_test, y_test, estimated_mean, estimated_cov)\n    loco_scores_ex4.append(loco_j)\n\nv_total = conditional_sage_value(N, model, X_test, y_test, estimated_mean, estimated_cov)\n\nfor j in range(p):\n    share = 100 * loco_scores_ex4[j] / v_total\n    print(f\"LOCO({feature_names[j]}): {loco_scores_ex4[j]:.4f}  ({share:.1f}% of explained variance)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Task 4: Plot LOCO scores\nv_total_plot = conditional_sage_value(N, model, X_test, y_test, estimated_mean, estimated_cov)\nloco_pct = [100 * s / v_total_plot for s in loco_scores_ex4]\n\nplt.figure(figsize=(6, 4))\nplt.barh(feature_names[::-1], loco_pct[::-1],\n         color='grey', edgecolor='black', linewidth=0.5)\nplt.xlabel(\"Share of explained variance (%)\")\nplt.title(\"LOCO (conditional marginalization)\")\nplt.axvline(0, color='black', linewidth=0.8)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Your interpretation (double-click to edit):**\n\n- Which features have non-zero LOCO values? Why?\n  - *Your answer here*\n\n- What fraction of the explained variance ($R^2$) is attributed to each feature?\n  - *Your answer here*\n\n- How do the LOCO scores compare to the CFI scores from Exercise 3?\n  - *Your answer here*\n\n- What does LOCO measure that CFI does not?\n  - *Your answer here*"
  }
 ]
}