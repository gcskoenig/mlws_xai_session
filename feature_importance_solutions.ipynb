{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Feature Importance Methods for Scientific Inference \u2014 SOLUTIONS\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\nRun all cells in this section before starting the exercises."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install fippy -q"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'font.size':        14,\n    'axes.titlesize':   16,\n    'axes.labelsize':   14,\n    'xtick.labelsize':  13,\n    'ytick.labelsize':  13,\n})"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Data\n\nWe generate a dataset with 5 features and a continuous target variable $Y$.\n**The data-generating process is hidden for now.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_data(n=1500, seed=83):\n    rng = np.random.RandomState(seed)\n    x1 = rng.normal(0, 1, n)\n    x2 = 0.999 * x1 + np.sqrt(1 - 0.999**2) * rng.normal(0, 1, n)\n    x3 = rng.normal(0, 1, n)\n    x4 = 0.999 * x3 + np.sqrt(1 - 0.999**2) * rng.normal(0, 1, n)\n    y  = 5 * x1 + rng.normal(0, 1, n)\n    x5 = rng.normal(0, 1, n)\n    X  = np.column_stack([x1, x2, x3, x4, x5])\n    return X, y\n\nX, y = generate_data()\nfeature_names = [\"X1\", \"X2\", \"X3\", \"X4\", \"X5\"]\nprint(f\"Dataset: {X.shape}, features: {feature_names}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Model\n\nWe train an OLS linear regression on 1000 training observations and evaluate on 500 held-out test observations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "n_train = 1000\nX_train, X_test = X[:n_train], X[n_train:]\ny_train, y_test = y[:n_train], y[n_train:]\n\nmodel = LinearRegression().fit(X_train, y_train)\nr2  = model.score(X_test, y_test)\nprint(f\"Test R\\u00b2: {r2:.3f}\")\nprint(f\"Coefficients: {np.round(model.coef_, 2)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### fippy setup\n\nWe use the [`fippy`](https://github.com/gcskoenig/fippy) package for Exercises 2 and 3.\nThe **Gaussian sampler** estimates the conditional distribution $P(X_j \\mid X_{-j})$ in closed form\nusing the multivariate normal assumption."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from fippy.explainers import Explainer\nfrom fippy.samplers import GaussianSampler\n\n# fippy requires pandas DataFrames\nX_train_df = pd.DataFrame(X_train, columns=feature_names)\nX_test_df  = pd.DataFrame(X_test,  columns=feature_names)\ny_train_s  = pd.Series(y_train, name='y')\ny_test_s   = pd.Series(y_test,  name='y')\n\nsampler   = GaussianSampler(X_train_df)\nexplainer = Explainer(model.predict, X_train_df,\n                      loss=mean_squared_error, sampler=sampler)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Exercise 1: PFI \u2014 Implementation and Interpretation\n\n**True DGP:** $Y = 5X_1 + \\varepsilon_Y$, with $X_2 \\approx X_1$ and $X_4 \\approx X_3$, and $X_5$ independent of everything."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def my_pfi(model, X, y, feature_idx, n_repeats=50, seed=42):\n    \"\"\"\n    Permutation Feature Importance for a single feature.\n\n    PFI_j = mean_r [ L(y, f(X_perm_r)) ] - L(y, f(X))\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    baseline_mse = mean_squared_error(y, model.predict(X))\n\n    perturbed_mses = []\n    for _ in range(n_repeats):\n        X_perm = X.copy()\n        X_perm[:, feature_idx] = rng.permutation(X[:, feature_idx])\n        perturbed_mses.append(mean_squared_error(y, model.predict(X_perm)))\n\n    return np.mean(perturbed_mses) - baseline_mse\n\n\n# Compute and plot PFI for all features\npfi_scores = [my_pfi(model, X_test, y_test, j) for j in range(len(feature_names))]\nfor name, score in zip(feature_names, pfi_scores):\n    print(f\"PFI({name}): {score:.4f}\")\n\nplt.figure(figsize=(6, 4))\nplt.barh(feature_names[::-1], pfi_scores[::-1], color='grey', edgecolor='black', linewidth=0.5)\nplt.xlabel(\"PFI (increase in MSE)\")\nplt.title(\"Permutation Feature Importance\")\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Scatterplot: X3 vs X4 before and after permuting X3\nrng = np.random.RandomState(42)\nX_perm = X_test.copy()\nX_perm[:, 2] = rng.permutation(X_test[:, 2])\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\naxes[0].scatter(X_test[:, 2], X_test[:, 3], alpha=0.3, s=10, color='grey')\naxes[0].set(xlabel=\"$X_3$\", ylabel=\"$X_4$\", title=\"Original: $(X_3, X_4)$\",\n            xlim=(-4,4), ylim=(-4,4))\n\naxes[1].scatter(X_perm[:, 2], X_perm[:, 3], alpha=0.3, s=10, color='grey')\naxes[1].set(xlabel=r\"$\\tilde{X}_3$ (permuted)\", ylabel=\"$X_4$\",\n            title=\"After permuting $X_3$\", xlim=(-4,4), ylim=(-4,4))\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Solution: Why is PFI misleading here?\n\nThe fitted coefficients are approximately $\\hat{\\beta} = [3.11,\\; 1.88,\\; -2.11,\\; 2.17,\\; 0.02]$.\nDue to the near-perfect correlation $\\rho(X_3, X_4) = 0.999$, OLS assigns large **opposing** coefficients\nto $X_3$ and $X_4$. In the original data these almost cancel. After permuting $X_3$, the cancellation\nbreaks, causing large prediction errors \u2014 which PFI misinterprets as feature importance.\n\nPFI measures **model reliance**, not association with $Y$. Non-zero PFI does not imply $X_j \\not\\perp Y$."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Exercise 2: Conditional Feature Importance (CFI)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Conditional Feature Importance via fippy\nex_cfi = explainer.cfi(X_test_df, y_test_s, nr_runs=10)\nex_cfi.hbarplot()\nplt.show()\n\nmeans, stds = ex_cfi.fi_means_stds()\nprint(\"\\nCFI scores:\")\nfor feat, m, s in zip(feature_names, means, stds):\n    print(f\"  {feat}: {m:.4f} \u00b1 {s:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Solution: CFI interpretation\n\nCFI correctly assigns near-zero importance to $X_2$ (redundant given $X_1$), $X_3$, $X_4$\n(irrelevant collinear), and $X_5$ (purely irrelevant). Only $X_1$ receives a non-zero score,\nreflecting that it is the only feature with a **conditional association** with $Y$ given all others.\n\n| Feature | Role | CFI |\n|---|---|---|\n| $X_1$ | Direct cause of $Y$ | **Non-zero** |\n| $X_2$ | Correlated with $X_1$, no direct effect | $\\approx 0$ |\n| $X_3, X_4$ | Irrelevant, mutually correlated | $\\approx 0$ |\n| $X_5$ | Purely irrelevant | $\\approx 0$ |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Exercise 3: Leave-One-Covariate-Out (LOCO)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute v(S) for all features and for each leave-one-out set\nN = feature_names  # full coalition\n\nex_vN = explainer.csagevf(S=list(N), X_eval=X_test_df, y_eval=y_test_s)\nmeans_N, _ = ex_vN.fi_means_stds()\nv_N = float(np.array(means_N).flatten()[0])\nprint(f\"v(all features) = {v_N:.4f}  [\u2248 Var(Y)\u00b7R\u00b2]\\n\")\n\nloco_scores = {}\nfor feat in N:\n    N_minus_j = [f for f in N if f != feat]\n    ex_vNj = explainer.csagevf(S=N_minus_j, X_eval=X_test_df, y_eval=y_test_s)\n    means_Nj, _ = ex_vNj.fi_means_stds()\n    v_Nj = float(np.array(means_Nj).flatten()[0])\n    loco_scores[feat] = v_N - v_Nj\n    print(f\"LOCO({feat}): {loco_scores[feat]:.4f}  \"\n          f\"({100 * loco_scores[feat] / v_N:.1f}% of explained variance)\")\n\n# Plot as share of explained variance\npct   = [100 * loco_scores[f] / v_N for f in N]\nplt.figure(figsize=(6, 4))\nplt.barh(N[::-1], pct[::-1], color='grey', edgecolor='black', linewidth=0.5)\nplt.xlabel(\"Share of explained variance (%)\")\nplt.title(\"LOCO (conditional marginalization)\")\nplt.axvline(0, color='black', linewidth=0.8)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Solution: LOCO interpretation\n\nLOCO expresses importance as a **fraction of the model's $R^2$**.\n\n- $X_1$ accounts for nearly 100% of explained variance \u2014 removing it collapses the model.\n- All other features contribute $\\approx 0\\%$, since the conditional distribution can\n  reconstruct their contributions from the remaining features.\n\nLOCO $\\neq 0$ is equivalent to $X_j \\not\\perp\\!\\!\\!\\perp Y \\mid X_{-j}$ (conditional dependence).\nUnlike CFI, it also quantifies **how much** of the explained variance each feature is responsible for."
  }
 ]
}