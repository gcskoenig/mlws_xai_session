{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Feature Importance Methods for Scientific Inference \u2014 SOLUTIONS\n\nThis notebook contains the **full solutions** (code and interpretations) for all three exercises.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\nRun the cells below to set everything up."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'font.size':        14,\n    'axes.titlesize':   16,\n    'axes.labelsize':   14,\n    'xtick.labelsize':  13,\n    'ytick.labelsize':  13,\n    'legend.fontsize':  13,\n})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_data(n=1500, seed=83):\n    \"\"\"Generate the dataset. The DGP is hidden for now.\"\"\"\n    rng = np.random.RandomState(seed)\n    x1 = rng.normal(0, 1, n)\n    x2 = 0.999 * x1 + np.sqrt(1 - 0.999**2) * rng.normal(0, 1, n)\n    x3 = rng.normal(0, 1, n)\n    x4 = 0.999 * x3 + np.sqrt(1 - 0.999**2) * rng.normal(0, 1, n)\n    y = 5 * x1 + rng.normal(0, 1, n)\n    x5 = rng.normal(0, 1, n)\n    X = np.column_stack([x1, x2, x3, x4, x5])\n    return X, y\n\nX, y = generate_data()\nfeature_names = [\"X1\", \"X2\", \"X3\", \"X4\", \"X5\"]\n\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Feature names: {feature_names}\")\nprint(f\"\\nFirst 5 rows of X:\")\nprint(np.round(X[:5], 2))\nprint(f\"\\nFirst 5 values of Y: {np.round(y[:5], 2)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split into train and test\nn_train = 1000\nX_train, X_test = X[:n_train], X[n_train:]\ny_train, y_test = y[:n_train], y[n_train:]\n\n# Train a Linear Regression (unregularized)\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Evaluate\ny_pred = model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nr2 = model.score(X_test, y_test)\nprint(f\"Test MSE: {mse:.3f}\")\nprint(f\"Test R\\u00b2:  {r2:.3f}\")\nprint(f\"\\nFitted coefficients: {np.round(model.coef_, 2)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def permutation_sampler(X, feature_idx, rng=None):\n    \"\"\"\n    Permutation (marginal) sampler.\n    Returns a copy of X where column `feature_idx` is randomly permuted,\n    effectively sampling X_j from its marginal distribution independently\n    of all other features and Y.\n    \"\"\"\n    if rng is None:\n        rng = np.random.RandomState(0)\n    X_perm = X.copy()\n    X_perm[:, feature_idx] = rng.permutation(X[:, feature_idx])\n    return X_perm\n\n\ndef compute_pfi(model, X, y, feature_idx, sampler, n_repeats=50, seed=42):\n    \"\"\"\n    Compute Permutation Feature Importance for feature `feature_idx`.\n\n    PFI_j = E[L(Y, f(X_tilde_j, X_{-j}))] - E[L(Y, f(X))]\n\n    Parameters\n    ----------\n    model : fitted sklearn model\n    X : np.ndarray, shape (n, p) - test features\n    y : np.ndarray, shape (n,) - test target\n    feature_idx : int - index of the feature to permute\n    sampler : callable - function(X, feature_idx, rng) -> X_perturbed\n    n_repeats : int - number of repetitions to average over\n    seed : int - random seed\n\n    Returns\n    -------\n    pfi_mean : float - mean PFI score across repeats\n    pfi_se   : float - standard error of the mean across repeats\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    baseline_mse = mean_squared_error(y, model.predict(X))\n\n    perturbed_mses = []\n    for _ in range(n_repeats):\n        X_perturbed = sampler(X, feature_idx, rng=rng)\n        y_pred_perturbed = model.predict(X_perturbed)\n        perturbed_mses.append(mean_squared_error(y, y_pred_perturbed))\n\n    pfi_values = np.array(perturbed_mses) - baseline_mse\n    pfi_mean = np.mean(pfi_values)\n    pfi_se = np.std(pfi_values) / np.sqrt(n_repeats)\n    return pfi_mean, pfi_se"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Pre-computed PFI Results\n\nThe plot below is what participants see before Exercise 1."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Pre-computed PFI (same as shown to participants)\npfi_scores_setup = [compute_pfi(model, X_test, y_test, feature_idx=j,\n                                sampler=permutation_sampler)\n                    for j in range(X_test.shape[1])]\n\nplt.figure(figsize=(6, 4))\nplt.barh(feature_names[::-1], pfi_scores_setup[::-1],\n         color='grey', edgecolor='black', linewidth=0.5)\nplt.xlabel(\"PFI (increase in MSE)\")\nplt.title(\"Permutation Feature Importance\")\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Solution 1: Interpret the PFI Plot\n\n**Which features does the model rely on?**\n\nPFI is non-zero for all five features. Permuting any of them increases the prediction error, so the model relies on all of them to some degree.\n\n**Which features appear important in the data?**\n\nBased on PFI alone, one would conclude that $X_1$ through $X_4$ are all associated with $Y$, with $X_1$ appearing most important and $X_5$ negligible.\n\n**What does PFI measure exactly?**\n\nPFI measures **model reliance**: how much the model's predictions degrade when the association between $X_j$ and all other variables is broken. A non-zero PFI means the model uses that feature \u2014 but this does **not** necessarily mean the feature is causally or even statistically associated with $Y$.\n\n> This corresponds to *model reliance* in Ewald et al. (2024), Section 5.1."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Solution 2: Why is PFI misleading?\n\n**The true DGP:**\n\n$$X_1 \\sim \\mathcal{N}(0,1), \\quad X_3 \\sim \\mathcal{N}(0,1), \\quad X_5 \\sim \\mathcal{N}(0,1) \\quad \\text{(mutually independent)}$$\n\n$$X_2 = 0.999 \\cdot X_1 + \\sqrt{1 - 0.999^2} \\cdot \\varepsilon_2, \\quad X_4 = 0.999 \\cdot X_3 + \\sqrt{1 - 0.999^2} \\cdot \\varepsilon_4$$\n\n$$Y = 5 X_1 + \\varepsilon_Y, \\quad \\varepsilon_Y \\sim \\mathcal{N}(0, 1)$$\n\n$Y$ depends **only** on $X_1$. $X_3$, $X_4$, and $X_5$ are **completely independent** of $Y$."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Task 1: Implement PFI from scratch\ndef my_pfi(model, X, y, feature_idx, n_repeats=50, seed=42):\n    rng = np.random.RandomState(seed)\n    baseline_mse = mean_squared_error(y, model.predict(X))\n    perturbed_mses = []\n    for _ in range(n_repeats):\n        X_perm = X.copy()\n        X_perm[:, feature_idx] = rng.permutation(X[:, feature_idx])\n        perturbed_mses.append(mean_squared_error(y, model.predict(X_perm)))\n    return np.mean(perturbed_mses) - baseline_mse\n\n# Verify \u2014 should match the pre-computed pfi_scores_setup\nfor j in range(X_test.shape[1]):\n    mine     = my_pfi(model, X_test, y_test, feature_idx=j)\n    provided = pfi_scores_setup[j]\n    print(f\"X{j+1}: mine={mine:.4f}  provided={provided:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create permuted versions\nrng = np.random.RandomState(42)\nX_test_perm_x2 = permutation_sampler(X_test, feature_idx=1, rng=rng)\nrng = np.random.RandomState(42)\nX_test_perm_x3 = permutation_sampler(X_test, feature_idx=2, rng=rng)\n\n# Scatterplots: original vs permuted for both pairs\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Row 1: (X1, X2)\naxes[0, 0].scatter(X_test[:, 0], X_test[:, 1], alpha=0.3, s=10, color='#2196F3')\naxes[0, 0].set_xlabel(\"$X_1$\"); axes[0, 0].set_ylabel(\"$X_2$\")\naxes[0, 0].set_title(\"Original: $(X_1, X_2)$\")\naxes[0, 0].set_xlim(-4, 4); axes[0, 0].set_ylim(-4, 4)\n\naxes[0, 1].scatter(X_test_perm_x2[:, 0], X_test_perm_x2[:, 1], alpha=0.3, s=10, color='#FF9800')\naxes[0, 1].set_xlabel(\"$X_1$\"); axes[0, 1].set_ylabel(r\"$\\tilde{X}_2$ (permuted)\")\naxes[0, 1].set_title(\"After permuting $X_2$\")\naxes[0, 1].set_xlim(-4, 4); axes[0, 1].set_ylim(-4, 4)\n\n# Row 2: (X3, X4)\naxes[1, 0].scatter(X_test[:, 2], X_test[:, 3], alpha=0.3, s=10, color='#E91E63')\naxes[1, 0].set_xlabel(\"$X_3$\"); axes[1, 0].set_ylabel(\"$X_4$\")\naxes[1, 0].set_title(\"Original: $(X_3, X_4)$\")\naxes[1, 0].set_xlim(-4, 4); axes[1, 0].set_ylim(-4, 4)\n\naxes[1, 1].scatter(X_test_perm_x3[:, 2], X_test_perm_x3[:, 3], alpha=0.3, s=10, color='#9C27B0')\naxes[1, 1].set_xlabel(r\"$\\tilde{X}_3$ (permuted)\"); axes[1, 1].set_ylabel(\"$X_4$\")\naxes[1, 1].set_title(\"After permuting $X_3$\")\naxes[1, 1].set_xlim(-4, 4); axes[1, 1].set_ylim(-4, 4)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFitted coefficients: {np.round(model.coef_, 2)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Explanation (Solution 2)\n\n**What do you notice in the scatterplots?**\n\n- **Left column (original):** Both $(X_1, X_2)$ and $(X_3, X_4)$ show very tight ellipses along the diagonal \u2014 the features within each pair are nearly identical ($\\rho \\approx 0.999$).\n- **Right column (permuted):** After permuting one feature in each pair, the correlation is destroyed. The data forms circular clouds with many **data points that never occurred in the training data**.\n\n**How do the model coefficients explain the PFI scores?**\n\nThe fitted coefficients are approximately `[3.11, 1.88, -2.11, 2.17]`. Due to the extreme multicollinearity ($\\rho = 0.999$), OLS assigns **large opposing coefficients** within each correlated pair:\n- The $X_1$/$X_2$ pair: coefficients ~3.1 and ~1.9 (the true effect of 5 is split between them)\n- The $X_3$/$X_4$ pair: coefficients ~-2.1 and ~2.2 (these are **pure overfitting** \u2014 they approximately cancel each other out, but each is large individually)\n\nWhen you permute one feature in a pair, the near-perfect cancellation breaks, causing large prediction errors.\n\n**Why does PFI assign high importance to $X_3$ and $X_4$?**\n\n1. OLS assigns large but opposing coefficients to $X_3$ and $X_4$ (because they are nearly collinear and the coefficient estimates are unstable)\n2. In the original data, $X_3 \\approx X_4$, so the contributions of $\\hat{\\beta}_3 X_3$ and $\\hat{\\beta}_4 X_4$ nearly cancel\n3. When the permutation sampler breaks the $X_3$-$X_4$ correlation, this cancellation is destroyed, and the model's predictions are wildly wrong\n4. PFI detects this as \"importance\" \u2014 but it reflects the model's reliance on the $X_3$-$X_4$ correlation, **not** any association between these features and $Y$\n\nThis is an even more dramatic failure than the $X_2$ case: $X_3$ and $X_4$ are **completely independent** of $Y$, yet PFI assigns them substantial importance.\n\n> This illustrates the problem described in *Negative Result 5.1.2* of Ewald et al. (2024): non-zero PFI does not necessarily imply any association with $Y$."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Solution 3: Conditional Feature Importance (CFI)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def conditional_sampler(X, feature_idx, rng=None, mean=None, cov=None):\n    \"\"\"\n    Conditional sampler for multivariate normal data.\n    Samples X_j from X_j | X_{-j} using the closed-form Gaussian conditional.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (n, p)\n    feature_idx : int - the feature to resample\n    rng : np.random.RandomState\n    mean : np.ndarray, shape (p,) - mean of the joint distribution\n    cov : np.ndarray, shape (p, p) - covariance of the joint distribution\n\n    Returns\n    -------\n    X_cond : np.ndarray - copy of X with column `feature_idx` resampled conditionally\n    \"\"\"\n    if rng is None:\n        rng = np.random.RandomState(0)\n\n    n, p = X.shape\n    j = feature_idx\n\n    # Indices of all other features\n    others = [i for i in range(p) if i != j]\n\n    # Extract sub-matrices from the covariance\n    sigma_jj = cov[j, j]                              # scalar\n    sigma_j_others = cov[j, others]                    # (p-1,)\n    sigma_others_others = cov[np.ix_(others, others)]  # (p-1, p-1)\n\n    # Conditional parameters\n    sigma_others_inv = np.linalg.inv(sigma_others_others)\n    beta = sigma_j_others @ sigma_others_inv            # regression coefficients\n    cond_var = sigma_jj - sigma_j_others @ sigma_others_inv @ sigma_j_others  # conditional variance\n\n    # Conditional mean for each observation: mu_j + beta @ (x_{-j} - mu_{-j})\n    x_others = X[:, others]\n    cond_means = mean[j] + (x_others - mean[others]) @ beta  # (n,)\n\n    # Sample from conditional distribution\n    X_cond = X.copy()\n    X_cond[:, j] = cond_means + rng.normal(0, np.sqrt(max(cond_var, 0)), n)\n\n    return X_cond"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Estimate mean and covariance from training data\nestimated_mean = np.mean(X_train, axis=0)\nestimated_cov = np.cov(X_train, rowvar=False)\n\nprint(\"Estimated covariance matrix:\")\nprint(np.round(estimated_cov, 3))\nprint(\"\\nNotice: X1-X2 are highly correlated, X3-X4 are highly correlated,\")\nprint(\"but the two pairs are independent of each other.\")\n\n\ndef conditional_sampler_wrapper(X, feature_idx, rng=None):\n    \"\"\"Wrapper so conditional_sampler has the same signature as permutation_sampler.\"\"\"\n    return conditional_sampler(X, feature_idx, rng=rng,\n                               mean=estimated_mean, cov=estimated_cov)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute CFI for all features (50 permutations each)\ncfi_scores = []\ncfi_ses = []\nfor j in range(X_test.shape[1]):\n    cfi_mean, cfi_se = compute_pfi(model, X_test, y_test, feature_idx=j,\n                                   sampler=conditional_sampler_wrapper)\n    cfi_scores.append(cfi_mean)\n    cfi_ses.append(cfi_se)\n    print(f\"CFI({feature_names[j]}): {cfi_mean:.4f} \u00b1 {cfi_se:.4f}\")\n\n# Side-by-side horizontal bar charts with SE (X1 on top \u2192 reverse order for barh)\nfig, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=False)\n\nerr_kw = dict(ecolor='black', capsize=4, capthick=1.5, elinewidth=1.5)\nnames_r = feature_names[::-1]\n\naxes[0].barh(names_r, pfi_scores[::-1], xerr=pfi_ses[::-1], color='grey',\n             edgecolor='black', linewidth=0.5, error_kw=err_kw)\naxes[0].set_xlabel(\"Importance (increase in MSE)\")\naxes[0].set_title(\"PFI (Permutation / Marginal Sampler)\")\n\naxes[1].barh(names_r, cfi_scores[::-1], xerr=cfi_ses[::-1], color='grey',\n             edgecolor='black', linewidth=0.5, error_kw=err_kw)\naxes[1].set_xlabel(\"Importance (increase in MSE)\")\naxes[1].set_title(\"CFI (Conditional Sampler)\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optional: Scatterplot comparison for both pairs\nrng = np.random.RandomState(42)\nX_test_cond_x2 = conditional_sampler_wrapper(X_test, feature_idx=1, rng=rng)\nrng = np.random.RandomState(42)\nX_test_cond_x3 = conditional_sampler_wrapper(X_test, feature_idx=2, rng=rng)\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 9))\n\n# Row 1: (X1, X2) \u2014 original, permuted, conditional\nfor ax in axes[0]: ax.set_xlim(-4, 4); ax.set_ylim(-4, 4)\naxes[0, 0].scatter(X_test[:, 0], X_test[:, 1], alpha=0.3, s=10, color='#2196F3')\naxes[0, 0].set_xlabel(\"$X_1$\"); axes[0, 0].set_ylabel(\"$X_2$\")\naxes[0, 0].set_title(\"Original data\")\n\naxes[0, 1].scatter(X_test_perm_x2[:, 0], X_test_perm_x2[:, 1], alpha=0.3, s=10, color='#FF9800')\naxes[0, 1].set_xlabel(\"$X_1$\"); axes[0, 1].set_ylabel(r\"$\\tilde{X}_2$\")\naxes[0, 1].set_title(\"Permutation sampler\")\n\naxes[0, 2].scatter(X_test_cond_x2[:, 0], X_test_cond_x2[:, 1], alpha=0.3, s=10, color='#4CAF50')\naxes[0, 2].set_xlabel(\"$X_1$\"); axes[0, 2].set_ylabel(r\"$\\tilde{X}_2$\")\naxes[0, 2].set_title(\"Conditional sampler\")\n\n# Row 2: (X3, X4) \u2014 original, permuted, conditional\nfor ax in axes[1]: ax.set_xlim(-4, 4); ax.set_ylim(-4, 4)\naxes[1, 0].scatter(X_test[:, 2], X_test[:, 3], alpha=0.3, s=10, color='#E91E63')\naxes[1, 0].set_xlabel(\"$X_3$\"); axes[1, 0].set_ylabel(\"$X_4$\")\naxes[1, 0].set_title(\"Original data\")\n\naxes[1, 1].scatter(X_test_perm_x3[:, 2], X_test_perm_x3[:, 3], alpha=0.3, s=10, color='#9C27B0')\naxes[1, 1].set_xlabel(r\"$\\tilde{X}_3$\"); axes[1, 1].set_ylabel(\"$X_4$\")\naxes[1, 1].set_title(\"Permutation sampler\")\n\naxes[1, 2].scatter(X_test_cond_x3[:, 2], X_test_cond_x3[:, 3], alpha=0.3, s=10, color='#4CAF50')\naxes[1, 2].set_xlabel(r\"$\\tilde{X}_3$\"); axes[1, 2].set_ylabel(\"$X_4$\")\naxes[1, 2].set_title(\"Conditional sampler\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Interpretation (Solution 3)\n\n**How do PFI and CFI differ?**\n\n- **PFI** assigns large importance to **all five features**, including $X_3$, $X_4$, and $X_5$ which are completely independent of $Y$.\n- **CFI** assigns a non-zero importance only to $X_1$. All other features have CFI $\\approx 0$.\n\nThe conditional sampler preserves the correlation within each pair. Since $X_2$ is still realistic given $X_1$ (and $X_4$ given $X_3$), the large opposing coefficients still cancel out, and the model's predictions remain accurate. For $X_5$, which is uncorrelated with everything, both samplers draw from the same marginal \u2014 yet any non-zero coefficient the model assigns will still inflate PFI.\n\n**Three types of features and how CFI treats them:**\n\nOur DGP contains three distinct types of features. CFI correctly identifies $X_1$ as the only important one, but it is worth understanding *why* each of the other features receives a CFI of zero \u2014 the reasons are different.\n\n| Feature | Role in DGP | Unconditionally associated with $Y$? | Conditionally associated with $Y$ (given all others)? | PFI | CFI |\n|---|---|---|---|---|---|\n| $X_1$ | **Directly relevant** \u2014 appears in the equation for $Y$ | Yes | Yes | High | **Non-zero** |\n| $X_2$ | **Indirectly relevant** \u2014 correlated with $X_1$, which causes $Y$ | Yes (through $X_1$) | **No** \u2014 once $X_1$ is known, $X_2$ adds nothing | High | $\\approx 0$ |\n| $X_3, X_4$ | **Collinear irrelevant** \u2014 no connection to $Y$, but mutually correlated | **No** | **No** | High | $\\approx 0$ |\n| $X_5$ | **Purely irrelevant** \u2014 independent of $Y$ and of all other features | **No** | **No** | Low | $\\approx 0$ |\n\nIn more detail:\n\n1. **$X_1$ \u2014 directly relevant (conditionally associated with $Y$):** $X_1$ appears in the equation $Y = 5 X_1 + \\varepsilon$. Even after conditioning on all other features, $X_1$ still provides unique information about $Y$ that cannot be recovered from the others. CFI is non-zero.\n\n2. **$X_2$ \u2014 indirectly relevant (unconditionally associated, but conditionally independent):** $X_2$ is correlated with $Y$ \u2014 but *only* because it is a noisy copy of $X_1$. In statistical terms, $X_2$ is *unconditionally associated* with $Y$ (i.e., $X_2 \\not\\perp\\!\\!\\!\\perp Y$), but *conditionally independent* of $Y$ given $X_1$ (i.e., $X_2 \\perp\\!\\!\\!\\perp Y \\mid X_1$). Once you know $X_1$, knowing $X_2$ tells you nothing new about $Y$. The conditional sampler preserves the $X_1$-$X_2$ relationship, so the model's predictions are unaffected. CFI $\\approx 0$.\n\n3. **$X_3, X_4$ \u2014 collinear irrelevant (unconditionally independent of $Y$):** $X_3$ and $X_4$ have no association with $Y$ at all \u2014 neither unconditionally nor conditionally. The conditional sampler preserves the $X_3$-$X_4$ correlation, so the opposing coefficients continue to cancel, and predictions are unaffected. CFI $\\approx 0$.\n\n4. **$X_5$ \u2014 purely irrelevant (independent of everything):** $X_5$ is independent of $Y$ and of all other features. OLS assigns a near-zero coefficient to it, so even the permutation sampler produces little error increase. Both PFI and CFI are close to zero \u2014 but $X_5$ serves as a useful **baseline**: any method that assigns large importance to $X_5$ is clearly misbehaving.\n\nNote that CFI gives $\\approx 0$ for $X_2$, $X_3$/$X_4$, and $X_5$, but for fundamentally different reasons:\n- $X_2$ is zero because it is **redundant** \u2014 its information about $Y$ is already captured by $X_1$\n- $X_3$/$X_4$ are zero because they are **collinearly irrelevant** \u2014 no information about $Y$, but the model's large canceling coefficients would fool PFI\n- $X_5$ is zero for both methods because it is **purely irrelevant** with no collinear structure to exploit\n\n> This distinction corresponds to the difference between *unconditional association* (A1) and *conditional association* (A2) in Ewald et al. (2024), Section 4. CFI specifically tests for conditional association: does $X_j$ provide information about $Y$ *beyond* what the other features already provide? Only $X_1$ passes this test.\n\n**What does this mean for scientific inference?**\n\nPFI and CFI answer fundamentally different questions:\n\n- **PFI** answers: *\"Which features does my model rely on?\"* This is a question about the model, not about the data. It can be misleading for scientific conclusions because it conflates true feature-target associations with broken feature-feature correlations.\n\n- **CFI** answers: *\"Which features have a unique, direct association with $Y$ that is not already captured by the other features?\"* This is more appropriate for scientific inference because it filters out both redundant features (like $X_2$) and completely irrelevant features (like $X_3$, $X_4$, $X_5$).\n\nNeither method is universally \"better\" \u2014 the right choice depends on what you want to learn.\n\n> See Ewald et al. (2024), Section 9 for practical recommendations on choosing between these approaches."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Solution 4: Leave-One-Covariate-Out (LOCO)\n\nLOCO asks: *how much does the model's performance drop if we remove feature $j$ entirely, marginalising it out with the conditional distribution?*\n\n## The conditional SAGE value function\n\nFor a coalition $S \\subseteq \\{1,\\ldots,p\\}$, define:\n\n$$v(S) = \\underbrace{\\mathbb{E}\\!\\left[(Y - \\mathbb{E}[f(X)])^2\\right]}_{\\text{baseline MSE}} - \\underbrace{\\mathbb{E}\\!\\left[(Y - \\mathbb{E}[f(X)\\mid X_S])^2\\right]}_{\\text{MSE using only } X_S}$$\n\nFor a **linear model** $f(X) = \\beta_0 + \\beta^\\top X$ with **Gaussian features**:\n\n$$\\mathbb{E}[f(X) \\mid X_S] = \\beta_0 + \\beta_S^\\top X_S + \\beta_{S^c}^\\top\\!\\left(\\mu_{S^c} + \\Sigma_{S^c,S}\\,\\Sigma_{S,S}^{-1}(X_S - \\mu_S)\\right)$$\n\n## LOCO\n\n$$\\text{LOCO}_j = v(\\{1,\\ldots,p\\}) - v(\\{1,\\ldots,p\\} \\setminus \\{j\\})$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from itertools import chain, combinations\nfrom math import factorial\n\ndef powerset(iterable):\n    s = list(iterable)\n    return chain.from_iterable(combinations(s, r) for r in range(len(s) + 1))\n\n\ndef conditional_sage_value(S, model, X, y, mean, cov):\n    \"\"\"\n    Conditional SAGE value function v(S).\n    v(S) = MSE_base - E[(Y - E[f(X)|X_S])^2]\n    Uses closed-form Gaussian conditional for linear models.\n    \"\"\"\n    p = X.shape[1]\n    S = list(S)\n    Sc = [j for j in range(p) if j not in S]\n    beta  = model.coef_\n    beta0 = model.intercept_\n\n    # Baseline: constant prediction E[f(X)] = beta0 + beta @ mean\n    f_base   = beta0 + beta @ mean\n    mse_base = np.mean((y - f_base) ** 2)\n\n    if len(S) == 0:\n        return 0.0\n\n    if len(Sc) == 0:\n        mse_full = np.mean((y - model.predict(X)) ** 2)\n        return mse_base - mse_full\n\n    # E[X_Sc | X_S] = mean_Sc + Sigma_{Sc,S} Sigma_{S,S}^{-1} (X_S - mean_S)\n    Sigma_Sc_S = cov[np.ix_(Sc, S)]\n    Sigma_S_S  = cov[np.ix_(S,  S)]\n    A          = Sigma_Sc_S @ np.linalg.inv(Sigma_S_S)\n    X_S        = X[:, S]\n    cond_mean_Sc = mean[Sc] + (X_S - mean[S]) @ A.T\n\n    f_S   = beta0 + X_S @ beta[S] + cond_mean_Sc @ beta[Sc]\n    mse_S = np.mean((y - f_S) ** 2)\n    return mse_base - mse_S"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "p = X_test.shape[1]\nN = tuple(range(p))\nv_total = conditional_sage_value(N, model, X_test, y_test, estimated_mean, estimated_cov)\n\n# \u2500\u2500 LOCO: v(N) - v(N \\ {j}) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nloco_scores = []\nfor j in range(p):\n    N_minus_j = tuple(i for i in range(p) if i != j)\n    loco_j = v_total - conditional_sage_value(N_minus_j, model, X_test, y_test,\n                                               estimated_mean, estimated_cov)\n    loco_scores.append(loco_j)\n    print(f\"LOCO({feature_names[j]}): {loco_j:.4f}  ({100*loco_j/v_total:.1f}% of explained var.)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot LOCO scores as share of explained variance\nloco_pct = [100 * s / v_total for s in loco_scores]\n\nplt.figure(figsize=(6, 4))\nplt.barh(feature_names[::-1], loco_pct[::-1],\n         color='grey', edgecolor='black', linewidth=0.5)\nplt.xlabel(\"Share of explained variance (%)\")\nplt.title(\"LOCO (conditional marginalization)\")\nplt.axvline(0, color='black', linewidth=0.8)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Interpretation (Solution 4)\n\n**What does LOCO measure?**\n\n$\\text{LOCO}_j = v(N) - v(N \\setminus \\{j\\})$ is the **drop in explained variance** when feature $j$ is removed from the full model, with the missing feature marginalised out using the conditional distribution.\n\nFor the L2-loss and an optimal model it equals $\\text{Var}(\\mathbb{E}[Y \\mid X_D]) - \\text{Var}(\\mathbb{E}[Y \\mid X_{D \\setminus j}])$, i.e. the reduction in explained variance.\n\n**Results for our DGP:**\n\n- **$X_1$**: LOCO $\\approx 100\\%$ of explained variance. Removing $X_1$ (and marginalising via the conditional distribution) collapses the model's predictive power almost entirely \u2014 as expected, since $X_1$ is the only true cause of $Y$.\n- **$X_2$**: LOCO $\\approx 0$. Once $X_1$ is available, $X_2$ adds nothing: $\\mathbb{E}[f(X) \\mid X_{-2}]$ can reconstruct the full prediction because $X_2 \\approx X_1$.\n- **$X_3, X_4$**: LOCO $\\approx 0$. Their large opposing coefficients still cancel when the conditional distribution is used.\n- **$X_5$**: LOCO $\\approx 0$. Purely irrelevant; removing it changes nothing.\n\n**Comparison with CFI:**\n\n| | CFI | LOCO |\n|---|---|---|\n| Question | Does $X_j$ improve predictions beyond the other features? | How much does $X_j$ contribute to total explained variance? |\n| $X_1$ | Non-zero | $\\approx 100\\%$ |\n| $X_2$ | $\\approx 0$ | $\\approx 0$ |\n| $X_3, X_4, X_5$ | $\\approx 0$ | $\\approx 0$ |\n| Magnitude | Increase in MSE | Fraction of $R^2$ |\n\nBoth CFI and LOCO correctly identify $X_1$ as the only relevant feature. LOCO additionally gives a **interpretable magnitude**: the fraction of the model's $R^2$ attributable to each feature.\n\n> LOCO $\\neq 0$ implies conditional dependence ($X_j \\not\\perp\\!\\!\\!\\perp Y \\mid X_{-j}$) and provides an interpretable measure of explained variance. See Ewald et al. (2024), Section 9."
  }
 ]
}